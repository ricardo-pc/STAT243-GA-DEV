
```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.tree import DecisionTreeRegressor
```

```{python}
import time

from sklearn.datasets import load_diabetes
X_diab, y_diab = load_diabetes(return_X_y=True, as_frame=True)

colnames = X_diab.columns
```

```{python}
t0 = time.time()
results = select(X_diab, y_diab, model_type="linear")
fulltime = time.time() - t0
print(results)
print(fulltime)
```

```{python}
df = pd.read_csv('baseball.dat', sep=r"\s+")
y_raw = df['salary'].values
y = np.log(y_raw) 
X = df.loc[:, df.columns != 'salary'].values
```

```{python}
t0 = time.time()
results = select(X, y, model_type="linear")
fulltime = time.time() - t0
print(results)
print(fulltime)
```

```{python}
def select(X, y, pred_names=None, penalty=None, model_type="linear", model_params=None, P=20, G=100, mut_rate=0.01):
    """Genetic Algorithm for variable selection.

    This function uses a genetic algorithm to do variable selection.
    The fitness metric is cross-validated R^2.

    Parameters
    ----------
    X: coerced to numpy array
        Predictor matrix (n samples by p predictors)
    y: coerced to numpy array
        Response vector (length n samples)
    pred_names: optional input, coerced to list
        Names of predictors. If None, generic names based on indexing are created.
    penalty: float
        Penalize fitness. Default is None.
    model_type: str
        linear (for linear regression, default), tree (for decision tree), or lasso (for Lasso regression)
    model_params: dict, optional input
        Model settings for random forest or lasso regression.
    P: int 
        Generation size. Default is 20.
    G: int 
        Number of generations. Default is 100. 
    mut_rate: float 
        Mutation rate. Default is 0.01 (1%).

    Returns
    -------
    result: dict
        A dictonary with: selected (index of best predictors), selected_names (names of best predictors), R2 (R^2 value of best model), R2pen (penalized R^2 value)
    """
    X = np.asarray(X)
    y = np.asarray(y)

    n, p = X.shape

    # If user doesn't pass predictor names, create generic ones
    if pred_names is None:
        pred_names = [f"x{j}" for j in range(p)]
    else:
        pred_names = list(pred_names)

    # Total sum of squares 
    SST = np.sum((y - np.mean(y))**2)

    # Main GA function
    best_chrom, best_main, best_aux = run_ga(
        X, y, penalty, model_type, model_params,
        SST, P, G, mut_rate
    )

    if penalty is None:
        R2 = float(best_main)
        R2pen = float(best_main)
    else:
        R2pen = float(best_main)
        R2 = float(best_aux)

    selected = np.flatnonzero(best_chrom).tolist()
    selected_names = [pred_names[j] for j in selected]

    result = {
        "selected": selected,
        "selected_names": selected_names,
        "R2": R2,
        "R2pen": R2pen
    }

    return result
```



```{python}
def run_ga(X, y, penalty, model_type, model_params, SST, P, G, mut_rate):
    """
    High-level Genetic Algorithm loop.
    """
    # K-fold splitter
    kf = KFold(n_splits=5, shuffle=True)
    folds = list(kf.split(X))

    # Create initial population (generation 0)
    # Each chromosome is a p-length vector of 0s and 1s
    # 1: include that predictor; 0: exclude that predictor 
    # Randomly choose 0 or 1 with probability 0.5 
    n, p = X.shape
    pop = (np.random.rand(P, p) < 0.5).astype(int)
    fitness_raw, fitness_pen = compute_fitness(pop, X, y, penalty, model_type, model_params, SST, folds)

    if penalty is None:
        main_fit = fitness_raw
        aux_fit = fitness_pen
    else:
        main_fit = fitness_pen
        aux_fit = fitness_raw

    # Track the global best fitness and chromosome
    # over all generations
    best_idx = main_fit.argmax()
    best_main = main_fit[best_idx]
    best_aux = aux_fit[best_idx]
    best_chrom = pop[best_idx]

    # Loop over generations 
    for gen in range(G):
        
        # Evolve population (new generation)
        pop = make_new_pop(pop, main_fit, mut_rate)

        # Evaluate fitness for new generation
        fitness_raw, fitness_pen = compute_fitness(pop, X, y, penalty, model_type, model_params, SST, folds)

        if penalty is None:
            main_fit = fitness_raw
            aux_fit = fitness_pen
        else:
            main_fit = fitness_pen
            aux_fit = fitness_raw

        # Track best fitness for this generation
        best_idx_gen = main_fit.argmax()
        best_main_gen = main_fit[best_idx_gen]

        # If this generation found a better model, update global bests
        if best_main_gen > best_main:
            best_main = best_main_gen
            best_aux = aux_fit[best_idx_gen]
            best_chrom = pop[best_idx_gen]

    return best_chrom, best_main, best_aux
```

```{python}
def compute_fitness(gen, X, y, penalty, model_type, model_params, SST, folds):
    """
    Compute the fitness for each chromosome in the current population.
    Where fitness is cross-validated R^2. 
    """
    P, p = gen.shape
    fitness_raw = np.zeros(P)
    fitness_pen = np.zeros(P)

    # Default settings for each model
    default_tree = {
        "max_depth": 5,
        "min_samples_split": 2,
        "min_samples_leaf": 5,
        "random_state": 42
    }

    default_lasso = {
        "alpha": 0.001,
        "max_iter": 5000,
        "tol": 1e-4,
        "random_state": 42
    }

    # Loop over chromosomes 
    for i in range(P):
        chrom = gen[i]
        k = chrom.sum()

        # If no predictors selected, give bad fitness 
        if k == 0:
            fitness_raw[i] = -1e9
            fitness_pen[i] = -1e9
            continue

        X_sel = X[:, chrom == 1]
        SSR = 0.0 # sum of squared residuals

        # Cross-validation loop 
        for train_idx, test_idx in folds:

            X_train = X_sel[train_idx]
            y_train = y[train_idx]

            X_test = X_sel[test_idx]
            y_test = y[test_idx]

            if model_type == "linear":
                model = LinearRegression()

            elif model_type == "tree":
                params = default_tree if model_params is None else model_params
                model = DecisionTreeRegressor(**params)

            elif model_type == "lasso":
                params = default_lasso if model_params is None else model_params
                model = Lasso(**params)

            # Fit model on training fold 
            model.fit(X_train, y_train)

            # Predict on test fold
            errors = y_test - model.predict(X_test)
            SSR += np.sum(errors**2)

        R2_raw = 1 - (SSR/SST)  # Cross-validated R2

        if penalty is None:
            R2_pen = R2_raw
        else:
            R2_pen = R2_raw - penalty*(k/p)

        fitness_raw[i] = R2_raw
        fitness_pen[i] = R2_pen

    return fitness_raw, fitness_pen
```

```{python}
def make_new_pop(gen, fitness, mut_rate):
    """
    Create a new generation from the current one.
    """
    P, p = gen.shape

    # 1. Rank-based selection to define how likely
    # each individual is to be chosen as a parent 
    idx_sorted = np.argsort(fitness)
    ranks = np.empty(P, float)
    ranks[idx_sorted] = np.arange(1, P+1)
    selection_prob = ranks/ranks.sum()

    # 2. Single-point crossover to mix parents into children
    pairs = P//2

    parent1_idx = np.random.choice(P, size=pairs, p=selection_prob)
    parent2_idx = np.random.randint(0, P, size=pairs)

    parent1 = gen[parent1_idx]
    parent2 = gen[parent2_idx]

    # crossover points
    cross_pts = np.random.randint(1, p, size=pairs)
    col_idx = np.arange(p)

    new_pop = np.zeros_like(gen)

    # create children 
    for k in range(pairs):
        cp = cross_pts[k]
        heads = (col_idx < cp)
        tails = ~heads

        # first child: parent1 head + parent2 tail
        new_pop[2*k] = parent1[k]*heads + parent2[k]*tails

        # second child: parent1 tail + parent2 head
        new_pop[2*k+1] = parent2[k]*heads + parent1[k]*tails

    # if P is odd, the last child is just a copy 
    # of the best-ranked (highest fitness) parent
    is_odd = (P % 2 == 1)
    if is_odd:
        best_parent_idx = idx_sorted[-1]
        new_pop[-1] = gen[best_parent_idx]

    # 3. Mutation: for each gene (bit), flip with small probability
    mutation_mask = (np.random.rand(P, p) < mut_rate)
    new_pop ^= mutation_mask

    return new_pop
```

